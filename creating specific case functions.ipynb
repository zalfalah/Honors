{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the tools we'll need\n",
    "import pandas as pd\n",
    "import shapely as sp\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import intake\n",
    "from shapely.geometry import MultiPoint\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from p4tools import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading catalog\n"
     ]
    }
   ],
   "source": [
    "tiles = io.get_tile_coordinates(update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create reduce checker with functions\n",
    "olddatafull = pd.read_csv(\"n_obj_df.csv\")  #old dataframe we're comparing to\n",
    "olddata=olddatafull[olddatafull['MY'] == 29] #only look at entries in mars year 29\n",
    "testregion = 'Macclesfield' #set a test region\n",
    "allregions = olddata.region_name.unique() #define all regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Michael's code, gets the polygon of an obsid\n",
    "def get_outer_polygon_for_obsid(obsid, return_hull=True):\n",
    "    tiles = pd.read_csv(\"tile_coordinates.csv\")\n",
    "    obsid_tiles = tiles.query(\"obsid == @obsid\")\n",
    "    cols = \"BodyFixedCoordinateX BodyFixedCoordinateY\".split()\n",
    "    coords = obsid_tiles[cols]\n",
    "    coords.columns = [\"x\", \"y\"]\n",
    "    mp = MultiPoint(list(zip(coords.x, coords.y)))\n",
    "    if return_hull:\n",
    "        return mp.convex_hull\n",
    "    else:\n",
    "        return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just gives intersection area between two polygons, docstring from .intersection just def function for .reduce later.\n",
    "def oneinter(pA, pB):\n",
    "    oneint = pA.intersection(pB)\n",
    "    return oneint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now with reduce, self made function\n",
    "#TODO: Write docstring\n",
    "def reducechecker(polygon_array):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    #This block below gets our combinations of polygons, works as intended\n",
    "    bucket2 = []\n",
    "    combos = []\n",
    "    iterable1 = range(len(polygon_array)) \n",
    "    p = len(polygon_array)\n",
    "    for n in iterable1:\n",
    "        if p > 0:\n",
    "            comb = list(combinations(iterable1,p))\n",
    "            for item in comb:\n",
    "                x = item\n",
    "                combos.append(x)\n",
    "            p = p-1\n",
    "        else:\n",
    "            end\n",
    "    \n",
    "    \n",
    "    for ele in combos:\n",
    "        #create a blank dictionary\n",
    "        d = {}\n",
    "        #create an empty bucket\n",
    "        bucket = []\n",
    "        for number in ele:\n",
    "            #where number would be (for example) polygon0\n",
    "            bucket.append(polygon_array[number])\n",
    "        #bucket now [poly0, poly1, poly2, poly4] if using comb[0] above\n",
    "        #give the dictionary the polygons used from bucket\n",
    "        d['elements_used'] = bucket\n",
    "        #create a multipolygon out of the polygons used\n",
    "        Multi = MultiPolygon(bucket)\n",
    "        #create a dictionary input of the overall multipolygon\n",
    "        d['multipolygon'] = Multi\n",
    "        #if theres only one polygon in our bucket, make the area that polygons area\n",
    "        if len(bucket) == 1:\n",
    "             d['combo_area'] = bucket[0].area\n",
    "        #if theres more than one polygon in the bucket, just .reduce on individual polygon intersections within the bucket\n",
    "        else:\n",
    "            r = functools.reduce(oneinter, bucket)\n",
    "            #call combo area the final (.reduce'd) polygon's intersection\n",
    "            d['combo_area'] = r.area\n",
    "        #Put our bucket together\n",
    "        bucket2.append(d)\n",
    "    #create dataframe from bucket2\n",
    "    reducedf = pd.DataFrame(bucket2)\n",
    "    #spit out that dataframe\n",
    "    return reducedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(region_list):\n",
    "    bucket = []\n",
    "    for obs in olddata[olddata['region_name'] == region].obsid.unique():\n",
    "        d2 = {}\n",
    "        d2['region_name'] = region\n",
    "        d2['Obsid_Used'] = obs\n",
    "        x = get_outer_polygon_for_obsid(obs)\n",
    "        area = x.area\n",
    "        d2['Obsid_Area'] = area\n",
    "        d2['polygon_shape'] = x\n",
    "        bucket.append(d2)\n",
    "    df1 = pd.DataFrame(bucket)\n",
    "    df3 = reducechecker(df1.polygon_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(df3): #rename intake, right now df3 to reimd what exactly goes in, make general\n",
    "    bucket2 = []\n",
    "    for ele in df3.index:\n",
    "        #print(ele)\n",
    "        polylist = df3.elements_used[ele]\n",
    "        bucket = []\n",
    "        for poly in polylist:\n",
    "            x = df1.Obsid_Used[df1['polygon_shape'] == poly].values[0]\n",
    "            print(x)\n",
    "            bucket.append(x)\n",
    "        bucket2.append(bucket)\n",
    "    df3['obsids_used'] = np.array(bucket2)\n",
    "    for i in range(len(df3.elements_used)): \n",
    "        df3.at[i, 'number_hirise'] = int(len(df3.elements_used[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(df3): #same as above, make input more general\n",
    "    for obslist in df3.obsids_used:\n",
    "        print(obslist)\n",
    "        bucket = []\n",
    "        for obs in obslist:\n",
    "            print(obs)\n",
    "            x = olddata.Ls[olddata['obsid'] == obs].values[0]\n",
    "            bucket.append(x)\n",
    "        bucket = tuple(bucket)\n",
    "        bucket2.append(bucket)\n",
    "    df3['Ls_used'] = np.array(bucket2)\n",
    "    df3['region_name'] = region\n",
    "    df3 = df3.drop(['elements_used'], axis = 1) \n",
    "    #df3.to_csv(f\"{region}_combos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macclesfield\n",
      "Starburst\n",
      "Manhattan_Classic\n"
     ]
    }
   ],
   "source": [
    "for region in allregions[0:3]:\n",
    "    #print region name for testing\n",
    "    print(region)\n",
    "    step1(allregions[0:3])\n",
    "    \n",
    "    #split up the bottom into two different functions or maybe 3 that take the output of the above as the input\n",
    "    \n",
    "    \n",
    "    #if knowledge must be kept alive into 2nd or 3rd function then we need a class.  \n",
    "    #example classes are in the thing he wrote.  Called coverage utils, class coverage calculator.  In that case\n",
    "    #the print note says obsid was set to a constant, then other things were done on an OBSID\n",
    "    #the class is set to an obsid, then all calculations use that obsid.  \n",
    "    #we determine a bucket, store it in a class, then the other functions just use it.  \n",
    "    #set just region name and probably df3\n",
    "    \n",
    "    #reduce checker creates df3, can pass that output into input of next function\n",
    "    \n",
    "    #####CHANGE NAMES OF VARIABLES#####\n",
    "    bucket2 = []\n",
    "    for ele in df3.index:\n",
    "        #print(ele)\n",
    "        polylist = df3.elements_used[ele]\n",
    "        bucket = []\n",
    "        for poly in polylist:\n",
    "            x = df1.Obsid_Used[df1['polygon_shape'] == poly].values[0]\n",
    "            print(x)\n",
    "            bucket.append(x)\n",
    "        bucket2.append(bucket)\n",
    "    df3['obsids_used'] = np.array(bucket2)\n",
    "    for i in range(len(df3.elements_used)): \n",
    "        df3.at[i, 'number_hirise'] = int(len(df3.elements_used[i]))\n",
    "    bucket2 = []\n",
    "    for obslist in df3.obsids_used:\n",
    "        print(obslist)\n",
    "        bucket = []\n",
    "        for obs in obslist:\n",
    "            print(obs)\n",
    "            x = olddata.Ls[olddata['obsid'] == obs].values[0]\n",
    "            bucket.append(x)\n",
    "        bucket = tuple(bucket)\n",
    "        bucket2.append(bucket)\n",
    "    df3['Ls_used'] = np.array(bucket2)\n",
    "    df3['region_name'] = region\n",
    "    df3 = df3.drop(['elements_used'], axis = 1) \n",
    "    df3.to_csv(f\"{region}_combos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-89efb45dbd16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObsid_Used\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'polygon_shape'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpoly\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'poly' is not defined"
     ]
    }
   ],
   "source": [
    "x = df1.Obsid_Used[df1['polygon_shape'] == poly].values[0] \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
